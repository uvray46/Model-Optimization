#Original Exercise

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix

# Set random seed for reproducibility
random_seed_number = 42
np.random.seed(random_seed_number)

# Load the diabetes data
data_path = r'C:\Users\jwhit\OneDrive\Documents\Data Science Course\KNN Grid Search\diabetes.csv'
diabetes_data = pd.read_csv(data_path)

# Review the data info
diabetes_data.info()

# Apply the describe function to the data
print(diabetes_data.describe())

# Replace zeros with NaN in specified columns
columns_with_zeros = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
diabetes_data[columns_with_zeros] = diabetes_data[columns_with_zeros].replace(0, np.nan)

# Plot histograms of each column before replacing NaN
diabetes_data.hist(figsize=(12, 10))
plt.suptitle('Histograms of Diabetes Data Columns (Before Imputation)')
plt.show()

# Replace NaNs with mean/median values
diabetes_data['Glucose'].fillna(diabetes_data['Glucose'].mean(), inplace=True)
diabetes_data['BloodPressure'].fillna(diabetes_data['BloodPressure'].mean(), inplace=True)
diabetes_data['SkinThickness'].fillna(diabetes_data['SkinThickness'].median(), inplace=True)
diabetes_data['Insulin'].fillna(diabetes_data['Insulin'].median(), inplace=True)
diabetes_data['BMI'].fillna(diabetes_data['BMI'].median(), inplace=True)

# Plot histograms of each column after replacing NaN
diabetes_data.hist(figsize=(12, 10))
plt.suptitle('Histograms of Diabetes Data Columns (After Imputation)')
plt.show()

# Plot the correlation matrix heatmap
plt.figure(figsize=(12, 10))
sns.heatmap(diabetes_data.corr(), annot=True, cmap='Blues')
plt.title('Correlation Matrix Heatmap')
plt.show()

# Define the y variable as the Outcome column
X = diabetes_data.drop(columns=['Outcome'])
y = diabetes_data['Outcome']

# Create a 70/30 train and test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_seed_number)

# Standardize the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Apply KNearestNeighbor classifier with neighbors 1-10
train_scores = []
test_scores = []

for i in range(1, 10):
    knn = KNeighborsClassifier(i)
    knn.fit(X_train_scaled, y_train)
    train_scores.append(knn.score(X_train_scaled, y_train))
    test_scores.append(knn.score(X_test_scaled, y_test))

# Print train and test scores
print("Train Scores:", train_scores)
print("Test Scores:", test_scores)

# Identify the number of neighbors with max scores
print("Best number of neighbors in training set:", np.argmax(train_scores) + 1)
print("Best number of neighbors in testing set:", np.argmax(test_scores) + 1)

# Plot the train and test model performance by number of neighbors
plt.figure(figsize=(12, 5))
sns.lineplot(x=range(1, 10), y=train_scores, marker='*', label='Train Score')
sns.lineplot(x=range(1, 10), y=test_scores, marker='o', label='Test Score')
plt.title('KNN Performance for Different Numbers of Neighbors')
plt.xlabel('Number of Neighbors')
plt.ylabel('Score')
plt.legend()
plt.show()

# Fit and score the best number of neighbors
knn = KNeighborsClassifier(np.argmax(test_scores) + 1)
knn.fit(X_train_scaled, y_train)
y_pred = knn.predict(X_test_scaled)
conf_matrix = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:\n", conf_matrix)

# Implement GridSearchCV to find the best n_neighbors parameter
param_grid = {'n_neighbors': np.arange(1, 50)}
knn = KNeighborsClassifier()
knn_cv = GridSearchCV(knn, param_grid, cv=5)
knn_cv.fit(X_train_scaled, y_train)

# Print the best score and best parameter for n_neighbors
print("Best Score:", knn_cv.best_score_)
print("Best Parameters:", knn_cv.best_params_)


#Random Forest Version

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix

# Set random seed for reproducibility
random_seed_number = 42
np.random.seed(random_seed_number)

# Load the diabetes data
data_path = r'C:\Users\jwhit\OneDrive\Documents\Data Science Course\KNN Grid Search\diabetes.csv'
diabetes_data = pd.read_csv(data_path)

# Replace zeros with NaN in specified columns
columns_with_zeros = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
diabetes_data[columns_with_zeros] = diabetes_data[columns_with_zeros].replace(0, np.nan)

# Replace NaNs with mean/median values
diabetes_data['Glucose'].fillna(diabetes_data['Glucose'].mean(), inplace=True)
diabetes_data['BloodPressure'].fillna(diabetes_data['BloodPressure'].mean(), inplace=True)
diabetes_data['SkinThickness'].fillna(diabetes_data['SkinThickness'].median(), inplace=True)
diabetes_data['Insulin'].fillna(diabetes_data['Insulin'].median(), inplace=True)
diabetes_data['BMI'].fillna(diabetes_data['BMI'].median(), inplace=True)

# Define the y variable as the Outcome column
X = diabetes_data.drop(columns=['Outcome'])
y = diabetes_data['Outcome']

# Create a 70/30 train and test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_seed_number)

# Standardize the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Implement GridSearchCV to find the optimal number of estimators in Random Forest
param_grid = {'n_estimators': np.arange(10, 200, 10)}  # Testing values from 10 to 190 in steps of 10
rf = RandomForestClassifier(random_state=random_seed_number)
rf_cv = GridSearchCV(rf, param_grid, cv=5)
rf_cv.fit(X_train_scaled, y_train)

# Print the best score and best parameter for n_estimators
print("Best Score:", rf_cv.best_score_)
print("Best Parameters:", rf_cv.best_params_)

# Fit the best model and evaluate on test data
best_rf = rf_cv.best_estimator_
best_rf.fit(X_train_scaled, y_train)
y_pred = best_rf.predict(X_test_scaled)
conf_matrix = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:\n", conf_matrix)

# Plot feature importances
plt.figure(figsize=(10, 6))
sns.barplot(x=best_rf.feature_importances_, y=X.columns)
plt.title('Feature Importances from Random Forest')
plt.show()
