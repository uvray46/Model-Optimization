! pip install bayesian-optimization lightgbm catboost

# Import necessary libraries
import warnings
warnings.filterwarnings('ignore')
from sklearn.preprocessing import LabelEncoder
import numpy as np
import pandas as pd
import lightgbm
from bayes_opt import BayesianOptimization
from catboost import CatBoostClassifier, cv, Pool
import os

# Check current working directory
os.listdir()

# Bayesian Optimization example with a simple function
def simple_func(a, b):
    return a + b
# Define the Bayesian optimizer and set bounds for the parameters
optimizer = BayesianOptimization(
    simple_func,
    {
        'a': (1, 3),
        'b': (4, 7)
    })
# Run the optimizer
optimizer.maximize(3, 2)

# Print the best result
print(optimizer.max['params']);optimizer.max['target']

# Load the flight delay dataset
train_df = pd.read_csv(r'C:\Users\jwhit\OneDrive\Documents\Data Science Course\Bayesian Optimization\flight_delays_train.csv')
test_df = pd.read_csv(r'C:\Users\jwhit\OneDrive\Documents\Data Science Course\Bayesian Optimization\flight_delays_test.csv')

# Review the dataset
print(train_df.head())
print(train_df.describe())

# Response variable mapping: Map 'Y' to 1 and 'N' to 0 in the 'dep_delayed_15min' column
y_train = train_df['dep_delayed_15min'].map({'Y': 1, 'N': 0}).values

# Define label encoding function
def label_enc(df_column):
    df_column = LabelEncoder().fit_transform(df_column)
    return df_column

# Define harmonic features
def make_harmonic_features_sin(value, period=2400):
    value *= 2 * np.pi / period 
    return np.sin(value)
def make_harmonic_features_cos(value, period=2400):
    value *= 2 * np.pi / period 
    return np.cos(value)

# Feature engineering function
def feature_eng(df):
    df['flight'] = df['Origin'] + df['Dest']
    df['Month'] = df.Month.map(lambda x: x.split('-')[-1]).astype('int32')
    df['DayofMonth'] = df.DayofMonth.map(lambda x: x.split('-')[-1]).astype('uint8')
    df['begin_of_month'] = (df['DayofMonth'] < 10).astype('uint8')
    df['midddle_of_month'] = ((df['DayofMonth'] >= 10) & (df['DayofMonth'] < 20)).astype('uint8')
    df['end_of_month'] = (df['DayofMonth'] >= 20).astype('uint8')
    df['DayOfWeek'] = df.DayOfWeek.map(lambda x: x.split('-')[-1]).astype('uint8')
    df['hour'] = df.DepTime.map(lambda x: x / 100).astype('int32')
    df['morning'] = df['hour'].map(lambda x: 1 if (x <= 11) & (x >= 7) else 0).astype('uint8')
    df['day'] = df['hour'].map(lambda x: 1 if (x >= 12) & (x <= 18) else 0).astype('uint8')
    df['evening'] = df['hour'].map(lambda x: 1 if (x >= 19) & (x <= 23) else 0).astype('uint8')
    df['night'] = df['hour'].map(lambda x: 1 if (x >= 0) & (x <= 6) else 0).astype('int32')
    df['winter'] = df['Month'].map(lambda x: x in [12, 1, 2]).astype('int32')
    df['spring'] = df['Month'].map(lambda x: x in [3, 4, 5]).astype('int32')
    df['summer'] = df['Month'].map(lambda x: x in [6, 7, 8]).astype('int32')
    df['autumn'] = df['Month'].map(lambda x: x in [9, 10, 11]).astype('int32')
    df['holiday'] = (df['DayOfWeek'] >= 5).astype(int) 
    df['weekday'] = (df['DayOfWeek'] < 5).astype(int)
    df['airport_dest_per_month'] = df.groupby(['Dest', 'Month'])['Dest'].transform('count')
    df['airport_origin_per_month'] = df.groupby(['Origin', 'Month'])['Origin'].transform('count')
    df['airport_dest_count'] = df.groupby(['Dest'])['Dest'].transform('count')
    df['airport_origin_count'] = df.groupby(['Origin'])['Origin'].transform('count')
    df['carrier_count'] = df.groupby(['UniqueCarrier'])['Dest'].transform('count')
    df['carrier_count_per month'] = df.groupby(['UniqueCarrier', 'Month'])['Dest'].transform('count')
    df['deptime_cos'] = df['DepTime'].map(make_harmonic_features_cos)
    df['deptime_sin'] = df['DepTime'].map(make_harmonic_features_sin)
    df['flightUC'] = df['flight'] + df['UniqueCarrier']
    df['DestUC'] = df['Dest'] + df['UniqueCarrier']
    df['OriginUC'] = df['Origin'] + df['UniqueCarrier']
    return df.drop('DepTime', axis=1)

# Concatenate the train and test dataframes for consistent feature engineering
full_df = pd.concat([train_df.drop('dep_delayed_15min', axis=1), test_df])
full_df = feature_eng(full_df)

# Apply label encoding to the necessary columns
for column in ['UniqueCarrier', 'Origin', 'Dest', 'flight',  'flightUC', 'DestUC', 'OriginUC']:
    full_df[column] = label_enc(full_df[column])

# Split the new full dataframe into X_train and X_test
X_train = full_df[:train_df.shape[0]]
X_test = full_df[train_df.shape[0]:]

# Define categorical features list
categorical_features = ['Month',  'DayOfWeek', 'UniqueCarrier', 'Origin', 'Dest', 'flight',  'flightUC', 'DestUC', 'OriginUC']

# Apply feature engineering to train and test data
train_df = feature_eng(train_df)
test_df = feature_eng(test_df)

# Check the engineered features
print("Train data after feature engineering:")
print(train_df.head())

# Define lightGBM evaluation function for Bayesian optimization
def lgb_eval(num_leaves, max_depth, lambda_l2, lambda_l1, min_child_samples, min_data_in_leaf):
    params = {
        "objective": "binary",
        "metric": "auc", 
        'is_unbalance': True,
        "num_leaves": int(num_leaves),
        "max_depth": int(max_depth),
        "lambda_l2": lambda_l2,
        "lambda_l1": lambda_l1,
        "num_threads": 20,
        "min_child_samples": int(min_child_samples),
        'min_data_in_leaf': int(min_data_in_leaf),
        "learning_rate": 0.03,
        "subsample_freq": 5,
        "bagging_seed": 42,
        "verbosity": -1
    }
    lgtrain = lightgbm.Dataset(X_train, y_train, categorical_feature=categorical_features)
    cv_result = lightgbm.cv(params, lgtrain, 1000, stratified=True, nfold=3)
    return cv_result['valid auc-mean'][-1]

# Bayesian optimization for lightGBM hyperparameter tuning
lgbBO = BayesianOptimization(lgb_eval, {
    'num_leaves': (25, 4000),
    'max_depth': (5, 63),
    'lambda_l2': (0.0, 0.05),
    'lambda_l1': (0.0, 0.05),
    'min_child_samples': (50, 10000),
    'min_data_in_leaf': (100, 2000)
})

# Maximize the lightGBM model with Bayesian Optimization
lgbBO.maximize(n_iter=5, init_points=2)

# Print best parameters from Bayesian optimization
print("Best parameters found by Bayesian Optimization:", lgbBO.max)

# Print the results of each iteration step
print("Step-by-step results from Bayesian Optimization:", lgbBO.res[0])
